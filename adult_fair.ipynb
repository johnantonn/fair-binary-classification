{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness-aware binary classification for the adult dataset\n",
    "This notebook works on the adult dataset: https://archive.ics.uci.edu/ml/datasets/adult. The dataset consists of circa 48k data points and 14 attributes, including two sensitive (or protected) attributes: \n",
    "- **sex**: {Female, Male}\n",
    "- **race**: {White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black}\n",
    "\n",
    "The target variable is binary and its goal is to predict whether an individual's annual income is <=50k or >50k. The focus of the notebook is the identification and correction of possible unfair treatment of individuals or groups, using the AIF360 toolset. To that aim, the below steps are followed:\n",
    "1. Data loading and fairness properties\n",
    "2. Fairness and bias-related exploratory data analysis\n",
    "3. Feature pre-processing/engineering\n",
    "4. Model training, bias removal and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm # progress bar\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from aif360.datasets import AdultDataset # inherits from StandardDataset\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "# inline browser plots for jupyter-notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single protected attribute: sex\n",
    "This section analyses the bias induced by the protected attribute 'sex'. From the attribute's values, 'Male' is considered the privileged class. The analysis takes into consideration only the numerical features of the dataset since they were found to result in a reasonably performant prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset properties\n",
    "protected = 'sex'\n",
    "privileged = ['Male']\n",
    "p = [{protected: 1}] # privileged group (males)\n",
    "u = [{protected: 0}] # unprivileged group (females)\n",
    "# Feature categories\n",
    "num_features = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week'] # numerical features\n",
    "cat_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'native-country'] # categorical features\n",
    "# Dataset instantiation\n",
    "ad = AdultDataset(\n",
    "    protected_attribute_names=[protected],\n",
    "    privileged_classes=[privileged],\n",
    "    categorical_features=[],\n",
    "    features_to_keep=num_features\n",
    ")\n",
    "# Verify that the dataset was initialized properly\n",
    "print('Features:', ad.feature_names)\n",
    "print('Target:', ad.label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairness-based exploratory data analysis\n",
    "#\n",
    "# Since the code is being used more than once in this notebook, it was defined as a function\n",
    "def analyze_fairness(dataset, df):\n",
    "    # Separate income by group\n",
    "    female_income = df[df['sex'] == 0]['income-per-year'] # female == 0\n",
    "    male_income = df[df['sex'] == 1]['income-per-year'] # male == 1\n",
    "    # Dataset metrics\n",
    "    dm = BinaryLabelDatasetMetric(dataset, privileged_groups=p, unprivileged_groups=u)\n",
    "    print('Base rate on entire population: {:.2f}'.format(dm.base_rate())) # privileged = None\n",
    "    print('Base rate on privileged group (males) :{:.2f}'.format(dm.base_rate(privileged=False)))\n",
    "    print('Base rate on unprivileged group (females) :{:.2f}'.format(dm.base_rate(privileged=True)))\n",
    "    print('Statistical parity difference: {:.2f}'.format(dm.mean_difference()))\n",
    "    print('Disparate impact: {:.2f}'.format(dm.disparate_impact()))\n",
    "    # Plots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5)) # [1 x 2] grid\n",
    "    ax1.bar(['<=50k', '>50k'], male_income.value_counts(), label='Male')\n",
    "    ax1.bar(['<=50k', '>50k'], female_income.value_counts(), bottom=male_income.value_counts(), label='Female')\n",
    "    ax1.set_xlabel('Income')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_title('Annual income grouped by sex')\n",
    "    ax1.legend()\n",
    "    ax2_y = df.groupby('sex')['income-per-year'].mean().sort_values(ascending=False)\n",
    "    ax2_y.plot.bar(x=['Male', 'Female'], color=['#1f77b4', '#ff7f0e'], ylim=[0, 1])\n",
    "    ax2.set_ylabel('Probability of income > 50k')\n",
    "    ax2.set_title('Probability of income > 50k grouped by sex')\n",
    "    ax2.set_xticklabels(['Male', 'Female'])\n",
    "    for i, v in enumerate(ax2_y):\n",
    "        ax2.text(i-0.07, v+0.02, '{:.2f}'.format(v))\n",
    "    plt.show()\n",
    "\n",
    "# Convert dataset to dataframe\n",
    "ad_df, _ = ad.convert_to_dataframe()\n",
    "# Call function\n",
    "analyze_fairness(ad, ad_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "- The **'sex'** attribute is dominated by 'Male' with 67% compared to 'Female' values with 33%. Note: the dominance of 'Male' value does not by itself indicate bias/unfair treatment.\n",
    "- The percentage of males with income > 50k w.r.t. total sample is 20%. The respective percentage of females is 4%.\n",
    "- The within-class percentage of females with income > 50k is 11%. The respective percentage for males is 30%.\n",
    "- A number of fairness metrics were calculated for the dataset at hand. More specifically, **statistical parity difference** and **disparate impact** indicate the existence of bias in the dataset.\n",
    "\n",
    "Note: **Base rate** is defined as $P(Y=1) = P/(P+N)$, optionally conditioned on protected attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature pre-processing/engineering\n",
    "#\n",
    "# Split dataset object to train and test partitions\n",
    "ad_train, ad_test = ad.split([0.7]) # 0.7 for train, 0.3 for test\n",
    "# Feature scaling\n",
    "scaler = MinMaxScaler() # choose min-max scaling\n",
    "ad_train.features = scaler.fit_transform(ad_train.features) # fit and transform train (seen) features\n",
    "ad_test.features = scaler.transform(ad_test.features) # transform test (unseen) features\n",
    "# Keep index of the protected attribute\n",
    "protected_idx = ad_train.feature_names.index(protected)\n",
    "# Verify training and test set dimensions\n",
    "print('Training set dimensions:', ad_train.features.shape)\n",
    "print('Test set dimensions:', ad_test.features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Single model evaluation\n",
    "The following section applies the Disparate Impact Remover algorithm provided in AIF360 to remove bias measured in terms of disparate impact. This algorithm is applied to the original dataset as a pre-processing step and edits the feature set to increase group fairness. It also provides a hyperparameter **repair_level** which is the repair amount (0.0 is no repair while 1.0 is full repair). Even though the focus is on disparate impact, the evaluation process includes an additional set of metrics w.r.t. (mis)classification rate differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single model training, bias removal and evaluation\n",
    "#\n",
    "# Logistic Regression\n",
    "#lr = LogisticRegression(solver='newton-cg') # Newton's conjugate gradient learning method\n",
    "lr = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
    "# Metric scores\n",
    "accuracy_list = [] # accuracy\n",
    "spd_list = [] # statistical parity difference\n",
    "di_list = []  # disparate impact\n",
    "fprd_list = [] # false positive rate difference\n",
    "fnrd_list = [] # false negative rate difference\n",
    "errd_list = [] # error rate difference\n",
    "#\n",
    "# Experiment with different repair levels\n",
    "for level in tqdm(np.arange(0., 1.1, 0.1)):\n",
    "    # Algorithms for bias removal\n",
    "    di_remover = DisparateImpactRemover(repair_level=level, sensitive_attribute='sex')\n",
    "\n",
    "    # Repaired versions of train and test dataset objects\n",
    "    ad_train_rep = di_remover.fit_transform(ad_train)\n",
    "    ad_test_rep = di_remover.fit_transform(ad_test)\n",
    "\n",
    "    # Construct X_train, X_test, y_train and y_test\n",
    "    X_train = np.delete(ad_train_rep.features, protected_idx, axis=1)\n",
    "    X_test = np.delete(ad_test_rep.features, protected_idx, axis=1)\n",
    "    y_train = ad_train_rep.labels.ravel()\n",
    "    y_test = ad_test_rep.labels.ravel()\n",
    "\n",
    "    # Training and prediction\n",
    "    lr.fit(X_train, y_train) # fit\n",
    "    ad_test_rep_pred = ad_test_rep.copy() # need both dataset objects due to their different labels\n",
    "    ad_test_rep_pred.labels = lr.predict(X_test) # prediction\n",
    "\n",
    "    # Dataset metric\n",
    "    dm = BinaryLabelDatasetMetric(ad_test_rep_pred, privileged_groups=p, unprivileged_groups=u)\n",
    "    di_list.append(dm.disparate_impact())\n",
    "    # Classification metric\n",
    "    cm = ClassificationMetric(ad_test_rep, ad_test_rep_pred, privileged_groups=p, unprivileged_groups=u)\n",
    "    # Metric calculations\n",
    "    accuracy_list.append(cm.accuracy())\n",
    "    fprd_list.append(cm.false_positive_rate_difference())\n",
    "    fnrd_list.append(cm.false_negative_rate_difference())\n",
    "    errd_list.append(cm.error_rate_difference())\n",
    "    spd_list.append(cm.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5)) # [1 x 2] grid\n",
    "ax1.plot(np.arange(0., 1.1, 0.1), di_list, marker='o')\n",
    "ax1.plot(np.arange(0., 1.1, 0.1), accuracy_list, marker='x')\n",
    "ax1.set_xlabel('Repair level')\n",
    "ax1.legend(['Disparate impact', 'Accuracy'])\n",
    "ax1.set_title('Disparate impact vs accuracy')\n",
    "ax2.plot(np.arange(0., 1.1, 0.1), fprd_list, marker='o')\n",
    "ax2.plot(np.arange(0., 1.1, 0.1), fnrd_list, marker='s')\n",
    "ax2.plot(np.arange(0., 1.1, 0.1), errd_list, marker='*')\n",
    "ax2.set_xlabel('Repair level')\n",
    "ax2.legend(['FPRD', 'FNRD', 'ERRD'])\n",
    "ax2.set_title('Differences of (mis)classification rates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "- The fairness-unaware performance of the Logistic Regression model is obtained with repair_level=0, for a specified set of model hyperparameters e.g. solver. For example, the newton-cg solver achieves better accuracy than liblinear solver, however the bias removal task is more challenging.\n",
    "- By increasing the **repair_level**, disparate impact score increases resulting to a fairer classifier; at the same time, the **accuracy** is not affected. For repair_level=1 we achieve the best disparate impact ratio equal to 0.95.\n",
    "- As far as **(mis)classification rate differences** are concerned, by removing disparate impact the respective FPRD and FNRD metrics are also reduced, up until a point (repair_level~0.4) where they reach a value close to 0. By increasing repair_level more than this threshold, the differences begin to increasea again, in the opposite direction. Finally, the ERRD is not reduced when treating for disparate impact.\n",
    "\n",
    "**Note**: brief explanation of the (mis)classification rate differences that are included in the second plot:\n",
    "- **FPRD** (False Positive Rate Difference): $FPR_{D=unprivileged}−FPR_{D=privileged}$\n",
    "- **FNRD** (False Negative Rate Difference): $FNR_{D=unprivileged}−FNR_{D=privileged}$\n",
    "- **ERRD** (Error Rate Difference): Difference in error rates for unprivileged and privileged groups, $ERR_{D=unprivileged}−ERR_{D=privileged}$, and $ERR=(FP+FN)/(P+N)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bias in repaired test set\n",
    "#\n",
    "# Create dataframe from test set\n",
    "sex = ad_test_rep_pred.features[:,protected_idx]\n",
    "inc = ad_test_rep_pred.labels\n",
    "ad_test_rep_pred_df = pd.DataFrame({'sex':sex, 'income-per-year':inc})\n",
    "# Call fairness analysis function\n",
    "analyze_fairness(ad_test_rep_pred, ad_test_rep_pred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Cross-model evaluation\n",
    "In this section, several classification models are applied to the problem and their performance both on accuracy and fairness is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-model evaluation\n",
    "#\n",
    "# Model definitions\n",
    "gnb = GaussianNB()\n",
    "knn = KNeighborsClassifier()\n",
    "lr1 = LogisticRegression(solver='newton-cg')\n",
    "lr2 = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
    "rf = RandomForestClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "models = [gnb, knn, lr1, lr2, rf, gb] # list of all model instances\n",
    "model_names = ['gnb', 'knn', 'lr1', 'lr2', 'rf', 'gb'] # names used for plotting\n",
    "\n",
    "# Metric scores\n",
    "accuracy_list = [] # accuracy\n",
    "di_list = []  # disparate impact\n",
    "fprd_list = [] # false positive rate difference\n",
    "fnrd_list = [] # false negative rate difference\n",
    "errd_list = [] # error rate difference\n",
    "\n",
    "# Algorithm for bias removal\n",
    "di_remover = DisparateImpactRemover(repair_level=1.0, sensitive_attribute='sex')\n",
    "# Repaired versions of train and test dataset objects\n",
    "ad_train_rep = di_remover.fit_transform(ad_train)\n",
    "ad_test_rep = di_remover.fit_transform(ad_test)\n",
    "\n",
    "# Construct X_train, X_test, y_train and y_test\n",
    "X_train = np.delete(ad_train_rep.features, protected_idx, axis=1)\n",
    "X_test = np.delete(ad_test_rep.features, protected_idx, axis=1)\n",
    "y_train = ad_train_rep.labels.ravel()\n",
    "y_test = ad_test_rep.labels.ravel()\n",
    "\n",
    "# Loop over models\n",
    "for model in models:\n",
    "    # Training and prediction\n",
    "    print('Training', type(model).__name__)\n",
    "    model.fit(X_train, y_train) # fit\n",
    "    ad_test_rep_pred = ad_test_rep.copy() # need both dataset objects due to their different labels\n",
    "    ad_test_rep_pred.labels = model.predict(X_test) # prediction\n",
    "    \n",
    "    # Dataset metric\n",
    "    dm = BinaryLabelDatasetMetric(ad_test_rep_pred, privileged_groups=p, unprivileged_groups=u)\n",
    "    di_list.append(dm.disparate_impact())\n",
    "    \n",
    "    # Classification metric\n",
    "    cm = ClassificationMetric(ad_test_rep, ad_test_rep_pred, privileged_groups=p, unprivileged_groups=u)\n",
    "    # Metric calculations\n",
    "    accuracy_list.append(cm.accuracy())\n",
    "    fprd_list.append(cm.false_positive_rate_difference())\n",
    "    fnrd_list.append(cm.false_negative_rate_difference())\n",
    "    errd_list.append(cm.error_rate_difference())\n",
    "    spd_list.append(cm.mean_difference())\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5)) # [1 x 2] grid\n",
    "# 1\n",
    "ax1.bar(range(len(models)), accuracy_list)\n",
    "ax1.bar(range(len(models)), di_list, alpha=0.75)\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_xlabel('Model')\n",
    "ax1.legend(['Accuracy', 'Disparate impact'])\n",
    "ax1.set(xlabel=\"Model\",title='Disparate impact vs accuracy', xticks=np.arange(len(models)), xticklabels=model_names)\n",
    "# 2\n",
    "ax2.plot(range(len(models)), fprd_list, marker='*', linestyle='dashed')\n",
    "ax2.plot(range(len(models)), fnrd_list, marker='o', linestyle='dashed')\n",
    "ax2.plot(range(len(models)), errd_list, marker='s', linestyle='dashed')\n",
    "ax2.set_xlabel('Model')\n",
    "ax2.legend(['FPRD', 'FNRD', 'ERRD'])\n",
    "ax2.set(xlabel=\"Model\",title='Differences of (mis)classification rates', xticks=np.arange(len(models)), xticklabels=model_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "- The experiments were conducted for several classification models/algorithms with bias removal using Disparate Impact Remover algorithm and a repair level of 1.0 (full repair).\n",
    "- Different classification algorithms result in different levels of accuracy and bias. For example, the logistic regression algorithm using the liblinear solver achieves very high disparate impact value (making it the fairest), but at a cost of accuracy which is significantly lower compared to the rest of the models. On the other hand, a member of the same family, i.e. logistic regression with newton-cg solver results in better accuracy but significantly lower value of disparate impact.\n",
    "- The second plot displays the (mis)classification metrics FPRD, FNRD and ERRD. The logistic regression model using liblinear solver achieves the best ERRD (however still non-zero).\n",
    "\n",
    "**Note**: The second graph should be interpreted as a discrete point plot and not as a continuous line-graph. The dashed lines were included to help visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
